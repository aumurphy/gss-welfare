---
title: "Welfare Survey Analysis"
author: "Austin Murphy"
date: "6/3/2020"
output: 
  html_document:
    toc: true
    toc_depth: 2
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE)

library(here)       # File location
library(ggplot2)    # Plotting
library(dplyr)      # Data Manip
library(BART)
```

Studying the Welfare data.

More info: http://gss.norc.org/Documents/reports/social-change-reports/SC61%20GSS_Trends%20in%20Spending_1973-2016.pdf

Variable info: https://gssdataexplorer.norc.org/variables/vfilter

**Covariates**
- wrkstat: Labor force status: Last week were you working full time, part time, going to school, keeping house, or what?
- prestg80: Respondents occupational prestige score (1980)
- hompop: Number of people in the household
  **Race questions**
On the average (Negroes/Blacks/African-Americans) have worse jobs, income, and housing than white people. Do you think these differences are mainly due to 
- racdif1: Discrimination?
- racdif2: Because most (Negroes/Blacks/African-Americans) have less in-born ability to learn?
- racdif3: Because most (Negroes/Blacks/African-Americans) don't have the chance for education that it takes to rise out of poverty?
- racdif4: Because most (Negroes/Blacks/African-Americans) just don't have the motivation or will power to pull themselves up out of poverty?
attblack is an average of the four variables above. 

**Outcome Variables**
Questions associated with this variable:
We are faced with many problems in this country, none of which can be solved easily or inexpensively. I'm going to name some of these problems, and for each one I'd like you to name some of these problems, and for each one I'd like you to tell me whether you think we're spending too much money on it, too little money, or about the right amount. First (READ ITEM A) . . . are we spending too much, too little, or about the right amount on (ITEM)?
- natfare: Welfare vs assistance to the poor. This is the name of the variable on the GSS website converted into Y in this dataset. 
- natrace{y}: Improving the conditions of Blacks VS Assistance to blacks


```{r loaddata}
# Downloading script
# df <- readr::read_csv(file = "https://raw.githubusercontent.com/gsbDBI/ExperimentData/master/Welfare/ProcessedData/welfarenolabel3.csv", na = character())
# readr::write_csv(df, here::here("data/welfare.csv"))

df <- readr::read_csv(file = here::here("data","welfare.csv"), na = character())
black_assistance <- readr::read_csv(file = here::here("data","black_assistance.csv"))
df <- 
  df %>% 
  dplyr::left_join(black_assistance)


# Specify outcome, treatment, and covariate variable names to use
outcome_variable_name <- "y" # or y_race for the other outcome variable
treatment_variable_name <- "w"
covariate_names <- c("partyid", "hrs1", "income", "rincome", "wrkstat", "wrkslf","age", "polviews",
                     "educ", "earnrs","wrkslf", "year", 
                     "marital","sibs","childs", "occ80",  "prestg80", "indus80","res16",
                     "reg16","mobile16", "family16", "parborn","maeduc","degree","sex",
                     "race","born","hompop","babies","preteen","teens","adults","attblack")


# Combine all names
all_variables_names <- c(outcome_variable_name, treatment_variable_name, covariate_names)
df <- df[, which(names(df) %in% all_variables_names)]

# Rename variables
names(df)[names(df) == outcome_variable_name] <- "Y"
names(df)[names(df) == treatment_variable_name] <- "W"

dim(df)
head(df)
colnames(df)
```


```{r data_cleaning}
# Tried a couple iterations of this to remove missing or 'other' values 
df <-
  df %>%
  filter(partyid != -999,  # missing
         partyid != 7,     # 7 represents "other party"
         polviews != -999, # missing
         polviews != 0,    # 0 represents "not applicable" according to norc.org
         year != 1987,     # year not included in paper
         attblack != -999, # missing
         educ != -999,     # missing
         income != -999)   # missing
```

```{r}
# Percent of rows that are -999 | NA
round(colSums(df == -999)/dim(df)[1],2)
colSums(df == -999)
```


```{r}
mean(df$Y[df$W == 1]) - mean(df$Y[df$W == 0])
# W==1: "Caring for the poor" wording
# W==0: "Welfare" wording. 

janitor::tabyl(df, year, W) %>%
  arrange(year) %>%
  janitor::adorn_totals()

```

I'm not sure why this doesn't match up with the Table 1 in the paper. 

Split into training / test

```{r}
set.seed(1)
N <- dim(df)[1]
num_train <- floor(N*.8)
train_idx <- sample(x=seq(N), size = num_train, replace = FALSE)

train <- df[train_idx,]
test <- df[-train_idx,]
```

GRF / causal forest:

```{r}
# library(grf)
# X <- as.matrix(train[,c(covariate_names)])
# Y <- train$Y
# W <- train$W
# 
# X.test <- as.matrix(test[,c(covariate_names)])
# Y.test <- test$Y
# W.test <- test$W
# 
# cf = grf::causal_forest(X=X, 
#                         Y=Y, 
#                         W=W, 
#                         num.trees = 100) 
# preds.cf = predict(cf, X.test)$predictions
```

```{r propensity_histogram}
# ggplot(data.frame(W.hat = cf$W.hat, W = factor(cf$W.orig))) +
#  geom_histogram(aes(x = W.hat, y = stat(density), fill = W), alpha=0.3, position = "identity") +
#  geom_density(aes(x = W.hat, color = W)) +
#  xlim(0,1) +
#  labs(title = "Causal forest propensity scores",
#       caption = "The propensity scores are learned via GRF's regression forest")
```

```{r}
# oob_pred <- predict(cf, estimate.variance=TRUE)
# head(oob_pred)
```


```{r}
# hist(preds.cf)
```


```{r}
####### BART
# bayesTrees - original package from paper authors
# bartMachine
# BART package - from authors
```

The paper used a probit BART model to estimate CATE because the response is binary. 

Can I replicate the findings in the paper?

```{r}
train <- df

summary(train$partyid)
table(train$partyid)

# prep 'test' data
n = dim(train)[1]
test0 = train %>% select(-Y); test0[ , 'partyid'] = 4; test0[ , 'W'] = 0
test1 = train %>% select(-Y); test1[ , 'partyid'] = 4; test1[ , 'W'] = 1

test_bart = rbind(test0, test1)

set.seed(1)
bart =  BART::pbart(x.train = as.data.frame(train %>% select(-Y)),
                    y.train = train$Y,
                    x.test = as.data.frame(test_bart),
                    nskip = 1000)

```

```{r}
dim(test_bart)
length(bart$prob.test.mean)

## turn z-scores into probabilities
bart$prob.test <- pnorm(bart$yhat.test)
## average over the posterior samples
bart$prob.test.mean <- apply(bart$prob.test, 2, mean)

## place estimates for arms 0-3 next to each other for convenience
itr <- cbind(bart$prob.test.mean[(1:n)], bart$prob.test.mean[n+(1:n)])

mean(itr[,1] - itr[,2])
itr_order = (itr[,1] - itr[,2])[order(itr[,1] - itr[,2])]
quantile(itr_order, 0.025)
quantile(itr_order, 0.975)
```

BART wants a `x.test` matrix to be passed in when training the model. To more easily create test matrices:

```{r}
# create covariate and treatment vectors
table(train$partyid)
covariate_levels = unique(train$partyid)[order(unique(train$partyid))]
num_levels = length(covariate_levels)
treatment_values = c(0,1)
test_design = cbind(rep(covariate_levels,each=2), rep(treatment_values,times=num_levels))

# prep 'test' data
n = dim(train)[1]
column_name = 'partyid'
test_bart = NULL
for (i in 1:dim(test_design)[1]){
  test0 = train %>% select(-Y)
  test0[ , column_name] = test_design[i,1]
  test0[ , 'W'] = test_design[i,2]
  
  test_bart=rbind(test_bart,test0)
}
dim(test_bart)

set.seed(1)
bart =  BART::pbart(x.train = as.data.frame(train %>% select(-Y)),
                    y.train = train$Y,
                    x.test = as.data.frame(test_bart),
                    nskip = 1000)
```

```{r}
## turn z-scores into probabilities
bart$prob.test <- pnorm(bart$yhat.test)
## average over the posterior samples
bart$prob.test.mean <- apply(bart$prob.test, 2, mean)

# Split each 'test' dataset
temp = rep(1:dim(test_design)[1],each=n)
list_split = split(bart$prob.test.mean, temp)

# Data for CATE, and lower and upper bound estimates
estimates = data.frame(matrix(NA,num_levels,4))
estimates[,1] = covariate_levels
names(estimates) <- c("covariate_val","mean","low","high")
for (i in 1:dim(estimates)[1]){
  covariate_val = 2*i-1
  cate = list_split[[covariate_val]] - list_split[[covariate_val+1]] 
  cate = cate[order(cate)]
  estimates[i,"mean"] = mean(cate)
  estimates[i,"low"] = quantile(cate, 0.025)
  estimates[i,"high"] = quantile(cate, 0.975)
}
estimates

ggplot(estimates, aes(x = covariate_levels, y = mean)) + 
  geom_line() + 
  geom_point() + 
  geom_ribbon(aes(ymin=low,ymax=high),alpha=0.3) + 
  ggtitle("CATE Estimate",column_name) + 
  xlab(column_name) + 
  ylab("CATE") +
  scale_x_continuous(breaks=covariate_levels,
        labels=c("Strong Dem","","","Independent","","","Strong Rep"))
```


